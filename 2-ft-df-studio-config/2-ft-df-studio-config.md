# Configura√ß√£o do Data Flow Studio no OCI Data Science

## Introdu√ß√£o

>‚ö†Ô∏è **ATEN√á√ÉO** ‚ö†Ô∏è
<br>
>**DOWNLOAD:** Fa√ßa o download do ZIP ([AQUI](https://objectstorage.sa-saopaulo-1.oraclecloud.com/p/aR4psHuDVUTRKxcK7ooD2JThAZg8ZrwHVM_qFKmSXsLSz_S_kXNkTBQ4QDOJy5VA/n/idi1o0a010nx/b/bucket-livelabs-engineering/o/livelabs.zip)), pois os arquivos ser√£o utilizados nos laborat√≥rios. Se voc√™ j√° realizou o download no primeiro laborat√≥rio, n√£o √© necess√°rio realizar novamente.
<br>
>**SENHA:** Durante o provisionamento dos recursos, √© necess√°rio a cria√ß√£o de senhas. Utilize SEMPRE a senha recomendada: **WORKSHOPsec2019##**
<br>
> **COMPARTMENT:** Realize todos os provisionamentos FORA DO compartimento **ROOT**. Considere a cria√ß√£o dos recursos no compartimento criado anteriormente.

Neste laborat√≥rio, voc√™ vai aprender a **configurar uma sess√£o no Data Flow Studio**, um componente integrado ao **OCI Data Science**.

O ambiente que ser√° configurado √© a **OCI Data Flow Session**, onde voc√™ poder√° **processar dados de forma interativa**, desenvolvendo, testando e otimizando fluxos em tempo real. 

### *O que √© o Data Flow?*

---

O **OCI Data Flow** √© um **servi√ßo totalmente gerenciado** da Oracle Cloud Infrastructure (OCI) que executa **scripts Apache Spark** sem a necessidade de instalar software ou gerenciar infraestrutura. Ele √© projetado para lidar com **grandes volumes de dados** de maneira simples e escal√°vel, permitindo que os usu√°rios se concentrem apenas na escrita de seus scripts.
Com ele, √© poss√≠vel criar aplica√ß√µes em **Java, Scala, Python ou SQL**, ideais para tarefas como:

* Processamento em lote de dados
* Opera√ß√µes de ETL (Extract, Transform, Load)

A principal vantagem do OCI Data Flow √© sua **escalabilidade e facilidade de uso**, acelerando o desenvolvimento sem preocupa√ß√µes com a infraestrutura subjacente.

### *O que √© o Data Flow Studio?*

---

J√° o **Data Flow Studio** √© uma **extens√£o integrada ao OCI Data Science**, que adiciona uma **camada interativa** para usar todo o poder do Data Flow em conjunto com outras ferramentas de ci√™ncia de dados.
Ele √© especialmente √∫til para realizar **opera√ß√µes de ETL via c√≥digo**, oferecendo flexibilidade para:

* Extrair dados de diferentes fontes,
* Transform√°-los conforme a necessidade do projeto,
* E carreg√°-los em destinos apropriados.

| **OCI Data Flow**                                                     | **OCI Data Flow Studio**                                                                   |
| --------------------------------------------------------------------- | ------------------------------------------------------------------------------------------ |
| Servi√ßo totalmente gerenciado para executar **scripts Apache Spark**. | Extens√£o do Data Flow integrada ao **OCI Data Science**.                                   |
| N√£o exige instalar software nem gerenciar infraestrutura.             | Oferece um **ambiente de desenvolvimento na nuvem (IDE)**.                                 |
| Escal√°vel para processar **grandes volumes de dados**.                | Permite **construir, testar e executar** aplica√ß√µes Spark de forma pr√°tica.                |
| Suporta aplica√ß√µes em **Java, Scala, Python e SQL** no Spark.         | Disponibiliza **interface gr√°fica** para intera√ß√£o mais intuitiva.                         |
| Ideal para **processamento em lote, ETL e an√°lises complexas**.       | Indicado para uma abordagem mais **visual e interativa** na cria√ß√£o de pipelines de dados. |

### *Observa√ß√£o:*

---

Quando voc√™ iniciar o notebook *`LiveLabs.ipynb`*, encontrar√° no sum√°rio uma rela√ß√£o completa dos assuntos que ser√£o explorados durante os laborat√≥rios 2 e 3.

> Para executar o c√≥digo de qualquer c√©lula no notebook, primeiro selecione a c√©lula que deseja executar. Em seguida, voc√™ pode pressionar *`SHIFT + ENTER`* no teclado ou clicar no bot√£o de execu√ß√£o (√≠cone de 'play') no notebook para realizar a a√ß√£o.  ![Execu√ß√£o Notebook](.\images\0-execution.png)

*Tempo estimado para o Lab:* 10 Minutos

### Objetivos

* Compreender a estrutura e funcionalidades de um notebook Jupyter.
* Aprender os atalhos de teclado e comandos espec√≠ficos do ambiente OCI Data Science.
* Dominar o processo de autentica√ß√£o usando o Oracle Accelerated Data Science SDK.
* Entender como configurar e utilizar vari√°veis de ambiente importantes no notebook.
* Aprender a usar comandos m√°gicos para intera√ß√£o com o Apache Spark.
* Desenvolver compet√™ncias para criar, monitorar e verificar o status de sess√µes do Spark.

## Aprendendo sobre Notebook Jupyter e comandos OCI Data Science

### *Como funciona um Notebook Jupyter:*

---

No √∫ltimo laborat√≥rio, importamos um **notebook Jupyter** para utiliz√°-lo no ambiente de Data Science. Mas afinal, **o que √© um notebook Jupyter e como ele funciona?**

Um notebook Jupyter √© uma das ferramentas mais populares entre **cientistas de dados, engenheiros e analistas**, pois combina **c√≥digo, explica√ß√µes em texto, gr√°ficos e visualiza√ß√µes** em um √∫nico documento interativo.

---

### **C√©lulas**

O notebook √© organizado em **blocos chamados c√©lulas**, que podem ser de dois tipos principais:

* **C√©lulas de C√≥digo:** permitem escrever e executar c√≥digo em linguagens como *Python, R ou PySpark*. Ao rodar a c√©lula (com `Shift + Enter`), o resultado aparece logo abaixo.
* **C√©lulas de Markdown:** usadas para escrever explica√ß√µes, adicionar imagens, links, tabelas ou listas. O Markdown √© uma linguagem de marca√ß√£o simples que permite formatar textos (negrito, it√°lico, listas, etc.) e at√© incluir trechos em HTML.

---

### **Execu√ß√£o Interativa**

O grande diferencial do Jupyter √© a **interatividade**. Voc√™ pode alterar um trecho de c√≥digo, executar apenas aquela c√©lula e ver o resultado imediatamente ‚Äî sem precisar rodar o notebook inteiro. Essa flexibilidade facilita testes, experimentos r√°pidos e aprendizado pr√°tico.

---

### **Versatilidade**

Os notebooks s√£o usados em diversas situa√ß√µes:

* **An√°lise de dados e visualiza√ß√µes**
* **Modelagem estat√≠stica e machine learning**
* **Cria√ß√£o de tutoriais, experimentos e guias educacionais**

---

### **Compartilhamento e Colabora√ß√£o**

Notebooks podem ser salvos no formato `.ipynb`, facilmente **compartilhados em plataformas como GitHub** ou executados em ambientes de nuvem, como o **OCI Data Science**, permitindo colabora√ß√£o entre equipes.

<br>

### *Comandos OCI Data Science*

---

Segue uma rela√ß√£o de comandos dispon√≠veis no ambiente do OCI Data Science. Para usar esses atalhos, voc√™ precisa estar em modo de comando (pressionando Esc antes do atalho):

| Atalho de Teclado               | A√ß√£o                                       |
|---------------------------------|--------------------------------------------|
| `Shift + Enter`                 | Executar a c√©lula atual e ir para a pr√≥xima|
| `Ctrl + Enter`                  | Executar a c√©lula atual                    |
| `Alt + Enter`                   | Executar a c√©lula atual e inserir uma nova |
| `Esc` e depois `A`              | Inserir c√©lula acima                       |
| `Esc` e depois `B`              | Inserir c√©lula abaixo                      |
| `Esc` e depois `D`, `D`         | Excluir c√©lula atual                       |
| `Esc` e depois `M`              | Mudar c√©lula atual para tipo Markdown      |
| `Esc` e depois `Y`              | Mudar c√©lula atual para tipo C√≥digo        |
| `Esc` e depois `L`              | Alternar n√∫meros de linha                  |
| `Esc` e depois `C`              | Copiar c√©lula                              |
| `Esc` e depois `X`              | Cortar c√©lula                              |
| `Esc` e depois `V`              | Colar c√©lula abaixo                        |
| `Esc` e depois `I`, `I`         | Interromper a execu√ß√£o da c√©lula           |


## Tarefa 1: Autentica√ß√£o

Para que a an√°lise de dados no Data Flow Studio seja segura e eficiente, √© fundamental configurar corretamente os m√©todos de autentica√ß√£o. Nesta se√ß√£o, vamos explorar o T√≥pico 1 do notebook, dedicado a esse processo.

O Oracle Accelerated Data Science SDK (ADS) controla o mecanismo de autentica√ß√£o com o cluster Spark em uma Sess√£o do OCI Data Flow. Para configurar a autentica√ß√£o, utilizamos *`ads.set_auth("resource_principal")`*

O par√¢metro *`resource_principal`* indica que est√° sendo utilizado um m√©todo de autentica√ß√£o espec√≠fico, associado a recursos e pol√≠ticas de seguran√ßa dentro da infraestrutura da Oracle Cloud. Assim, o c√≥digo est√° preparando o ambiente para que o usu√°rio possa interagir de forma segura com o Spark, garantindo que todas as opera√ß√µes realizadas estejam autenticadas e autorizadas de acordo com as configura√ß√µes do OCI Data Science.

1. **Selecione a c√©lula e execute-a com o comando SHIFT + ENTER, ou clique no bot√£o de execu√ß√£o (√≠cone de 'play') no notebook.**

    ![Realizando Autentica√ß√£o](.\images\1-authentication.png)

O c√≥digo abaixo realiza a autentica√ß√£o para usar servi√ßos da Oracle Cloud Infrastructure, especialmente o Data Flow Studio. Ele importa bibliotecas necess√°rias, incluindo o SDK da OCI, e configura um assinante de princ√≠pios de recursos, que permite fazer solicita√ß√µes autenticadas aos servi√ßos da OCI sem gerenciar manualmente as credenciais.

2. **Selecione a c√©lula e execute-a com o comando SHIFT + ENTER, ou clique no bot√£o de execu√ß√£o (√≠cone de 'play') no notebook.**

    ![Realizando Autentica√ß√£o](.\images\2-authentication-df.png)


## Tarefa 2: Cria√ß√£o de Vari√°veis

Este c√≥digo est√° configurando algumas vari√°veis importantes para serem utilizadas durante a *cria√ß√£o da sess√£o Spark*. 

1. **Selecione a c√©lula e execute-a com o comando SHIFT + ENTER, ou clique no bot√£o de execu√ß√£o (√≠cone de 'play') no notebook.**

    ![Vari√°veies Notebook](.\images\2-variables.png)

### *Informa√ß√£o sobre o compartment ID*

Primeiramente, ele importa a biblioteca *`os`*, que permite trabalhar com vari√°veis de ambiente do sistema operacional. Com isso, ele obt√©m o valor da vari√°vel de ambiente *`NB_SESSION_COMPARTMENT_OCID`*, que √© usada para identificar o compartimento espec√≠fico na Oracle Cloud Infrastructure (OCI) onde o notebook est√° armazenado. Esse valor √© ent√£o armazenado na vari√°vel *`compartment_id`*.

### *Informa√ß√£o sobre o bucket namespace*

Esse c√≥digo configura um cliente para acessar o servi√ßo de armazenamento de objetos da Oracle Cloud Infrastructure (OCI) e obt√©m o namespace do bucket de armazenamento. O cliente √© inicializado com configura√ß√µes padr√£o e autentica√ß√£o baseada no assinante configurado anteriormente. Em seguida, ele chama o m√©todo para obter o *namespace do bucket*, que √© um identificador √∫nico usado na OCI para organizar e separar recursos de armazenamento.

### *Informa√ß√£o sobre o nome dos buckets*

O c√≥digo lista todos os buckets em um namespace e compartimento espec√≠ficos da Oracle Cloud Infrastructure. Em seguida, ele procura e armazena os nomes dos primeiros buckets encontrados que cont√™m 'logs' e 'conda' em seus nomes, respectivamente, em duas vari√°veis diferentes.

### *Informa√ß√£o sobre o Metastore ID*

Esta etapa do c√≥digo configura um cliente para o Data Catalog da OCI, lista os metastores em um compartimento especificado e extrai o ID do primeiro metastore encontrado, armazenando-o em uma vari√°vel.

## Tarefa 3: Data Flow Spark Magic

### *Apache Spark e Data Flow*

O **Apache Spark** √© uma plataforma *open source* (c√≥digo aberto) criada para o **processamento de grandes volumes de dados**. Por ser livre e modific√°vel, conta com uma comunidade ativa que contribui constantemente para seu aprimoramento.

No **OCI Data Flow**, servi√ßo da Oracle Cloud Infrastructure, o Spark √© o **motor de processamento de dados que funciona por tr√°s das opera√ß√µes**. Ou seja, sempre que voc√™ executa an√°lises, processa grandes quantidades de dados (*big data*) ou roda opera√ß√µes complexas dentro do Data Flow, est√° utilizando a capacidade do Spark.

Uma das maiores vantagens do Spark √© sua habilidade de **distribuir tarefas entre v√°rios computadores em um cluster**, acelerando o processamento. Isso o torna perfeito para ambientes em nuvem, como o OCI, onde **escalabilidade e efici√™ncia** s√£o fundamentais.

Com essa integra√ß√£o, o OCI Data Flow permite que usu√°rios executem **processamento de dados em larga escala, an√°lises estat√≠sticas avan√ßadas e algoritmos de machine learning** de forma simples e eficiente, aproveitando toda a robustez da infraestrutura da Oracle Cloud.

### *Helpers*

---

O c√≥digo apresentado define uma fun√ß√£o auxiliar chamada `prepare_command`, uma ferramenta de conveni√™ncia para transformar argumentos que s√£o armazenados em vari√°veis Python ou estruturas de dados em uma forma que pode ser utilizada pelos comandos m√°gicos do Spark, que muitas vezes esperam argumentos em formato de string.

1. **Selecione a c√©lula e execute-a com o comando SHIFT + ENTER, ou clique no bot√£o de execu√ß√£o (√≠cone de 'play') no notebook.**

    ![C√≥digo Helpers](.\images\3-helpers.png)

Esses comandos m√°gicos permitem interagir de maneira eficiente com o Spark, facilitando a execu√ß√£o de opera√ß√µes complexas de processamento de dados de forma mais simplificada e integrada dentro do ambiente do notebook Jupyter. 

### *Data Flow Spark Magic*

---

O Data Flow Spark Magic refere-se a uma cole√ß√£o de "comandos m√°gicos" especializados no Jupyter Notebook que facilitam a intera√ß√£o com *Apache Spark*. Esses comandos s√£o poss√≠veis gra√ßas √† integra√ß√£o com a API REST do Apache Livy, um servi√ßo que permite a submiss√£o de trabalhos Spark de forma remota.

> O c√≥digo Spark escrito nos notebooks Jupyter √© executado em clusters Spark hospedados remotamente, permitindo lidar com grandes conjuntos de dados e tarefas de processamento intensivo.

Voc√™ precisa ativar o  Data Flow Spark Magic em seu notebook usando o comando m√°gico *`%load_ext dataflow.magics.`*

2. **Selecione a c√©lula e execute-a com o comando SHIFT + ENTER, ou clique no bot√£o de execu√ß√£o (√≠cone de 'play') no notebook.**

    ![C√≥digo Data Flow Magic](.\images\4-load-spark-magic.png)

Ap√≥s a ativa√ß√£o da extens√£o, o comando *`%help`* pode ser usado para obter a lista de todos os comandos dispon√≠veis, juntamente com uma lista de seus argumentos e exemplos de chamadas.

3. **Selecione a c√©lula e execute-a com o comando SHIFT + ENTER, ou clique no bot√£o de execu√ß√£o (√≠cone de 'play') no notebook**.

    ![C√≥digo Helpers](.\images\5-help.png)

> **[OPCIONAL] Doctring**: Para entender o prop√≥sito e os argumentos de um comando m√°gico no Jupyter Notebook, voc√™ pode adicionar um *`?`* ao final do comando, o que exibir√° a docstring, um texto explicativo incorporado no c√≥digo que fornece detalhes sobre sua funcionalidade. ![C√≥digo Docstring](.\images\1-callout-docstring.png)

## Tarefa 4: Criando uma Sess√£o

Neste momento do tutorial, vamos reutilizar algumas vari√°veis que foram criadas anteriormente na **Tarefa 3**. Vamos integr√°-las na cria√ß√£o da sess√£o do Data Flow Studio.

Para criar uma nova sess√£o de cluster do Data Flow, vamos utilizar o comando m√°gico *`%create_session`*.
De forma geral, este comando est√° configurando e iniciando uma nova sess√£o de cluster do Data Flow no OCI Data Science, especificando detalhes como o tipo de m√°quina virtual, configura√ß√µes de CPU e mem√≥ria, vers√£o do Spark, e onde armazenar os logs. 

1. **Selecione cada uma das c√©lulas e execute-as com o comando SHIFT + ENTER, ou clique no bot√£o de execu√ß√£o (√≠cone de 'play') no notebook**.

    ![Criando sess√£o](.\images\6-create-session.png)

    ![Criando sess√£o comando](.\images\7-create-session-command.png)

Vamos decompor cada parte para uma explica√ß√£o mais simples:

### *Comando M√°gico %create_session:*

---

Este √© um comando especial Data Flow Magic usado no Jupyter Notebook para iniciar uma nova sess√£o do Data Flow.

Op√ß√µes do Comando:

- *-l python:* Especifica a linguagem de programa√ß√£o a ser usada, neste caso, Python.
- *-c '{...}':* Cont√©m a configura√ß√£o em formato JSON para a sess√£o do Data Flow.

#### *Configura√ß√µes da Sess√£o:*

---

- *"compartmentId":* Identificador do compartimento na OCI onde a sess√£o ser√° criada.
- *"displayName":* Nome da sess√£o para identifica√ß√£o. Pode ser alterado livremente.
- *"language":* Define a linguagem de programa√ß√£o (Python).
- *"sparkVersion":* Especifica a vers√£o do Spark a ser usada.
- *"numExecutors":* N√∫mero de executores para o Spark. Os executores s√£o processos que executam tarefas e armazenam dados para o aplicativo Spark.
- *"driverShape" e "executorShape":* Especificam o tipo de m√°quina virtual (VM.Standard.E4.Flex) usada para o driver e os executores, respectivamente. O driver √© o processo central que coordena as tarefas do Spark.
- *"driverShapeConfig" e "executorShapeConfig":* Configura√ß√µes detalhadas para o driver e os executores, incluindo o n√∫mero de CPUs (OCPUs) e a quantidade de mem√≥ria (em GBs).
- *"logsBucketUri":* Endere√ßo do bucket (cont√™iner de armazenamento) na OCI onde os logs da sess√£o ser√£o armazenados.
- *"configuration":* Configura√ß√µes adicionais, como a localiza√ß√£o do ambiente do Spark e outras op√ß√µes de configura√ß√£o.
- *"metastoreId":* Um identificador para o metastore de dados usado. Um metastore √© um reposit√≥rio central para armazenar metadados sobre as estruturas de dados, como tabelas e esquemas, dentro do ambiente Spark.

2. **Depois de executar a c√©lula que cont√©m o comando m√°gico, aguarde a cria√ß√£o da sess√£o Spark.**

    ![Progresso Cluster](.\images\8-progress-cluster.png)

    A sess√£o do Spark estar√° pronta para ser utilizada assim que a seguinte mensagem for exibida.

    **ATEN√á√ÉO: Este processo pode demorar alguns minutos (15-25 min).**

    Cada vez que voc√™ cria uma nova sess√£o, um novo "Session ID" √© atribu√≠do a essa sess√£o, permitindo que o ambiente de cluster diferencie entre m√∫ltiplas sess√µes que podem estar ocorrendo simultaneamente. Esse ID pode ser utilizado para retomar, gerenciar ou encerrar a sess√£o espec√≠fica a qualquer momento.

    ![Cluster Pronto](.\images\9-cluster-ready.png)

    > **OBSERVA√á√ÉO:** N√£o se preocupe se n√£o aparecer a barra de progresso como na imagem abaixo, a indica√ß√£o que est√° sendo criado √© o asterisco (*) no lado esquerdo da c√©lula. Aguarde at√© que ele seja substitu√≠do por um n√∫mero, indicando que a c√©lula foi processada.

    ![Widget-fail](images/widget.png)

3. **Utilize o comando m√°gico *`%status`* para verificar o status da sess√£o atual.**

    ![Status Cluster](.\images\10-status-cluster.png)

> **Nota:** Os clusters da Sess√£o do OCI Data Flow ficam ativos por 24 horas (1440 minutos) por padr√£o, por√©m voc√™ pode customizar esse per√≠odo para criar sess√µes que permanecer√£o ativas por at√© 7 dias (10,080 minutos) (maxDurationInMinutes).

Parab√©ns, voc√™ terminou esse laborat√≥rio! üéâ

Voc√™ pode **seguir para o pr√≥ximo Lab**.

## Conclus√£o

Nesta laborat√≥rio, voc√™ aprendeu como utilizar notebooks Jupyter no OCI Data Science para criar e gerenciar sess√µes do Data Flow com Apache Spark, abrangendo desde autentica√ß√£o e configura√ß√£o de vari√°veis at√© a execu√ß√£o interativa de c√≥digo e monitoramento do status do cluster.

## Autoria

- *Created By/Date* - Thais Henrique, Heloisa Escobar, Isabelle Anjos, Janeiro 2024
- *Last Updated By* - Isabelle Anjos, Outubro 2025